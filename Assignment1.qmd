---
title: "STA5073Z Assignment 1"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(stringr)
library(dplyr)
library(tidyverse)
library(rpart)
library(gridExtra)
library(ggplot2)

#install.packages("tidytext")
library(tidytext)

#update r and install git
#install.packages("remotes")
#remotes::install_github(sprintf("rstudio/%s",
                        #c("reticulate", "tensorflow", "keras")))
#keras:: install_keras()
library(keras)
library(tensorflow)
#tf$constant("Hello world")
```

# Abstract

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

\*\*index words\*\* : Bag-of_words, Tf-IDF, Feed forward neural network (FFNN), random forest, classification tree

## Introduction

The aim of this paper is to identify different machine learning models that can be used for predictive purposes, and rank them according to their ability to do so. With presidential speech from 1994 to 2022 from South African presidents, this paper will review the capacity in which classification trees , feed forward neural networks and random forests are able to accurately classify sentences to their presidents. The input structuring process will also vary between bag of words and tf-idf so as to ascertain which one is more suitable for the data at hand with a selected machine learning model.

## Exploratory Data analysis

```{r Reading in Data}

set.seed(2022)
load("F:/STA5073Z- DS4I/pre-processed assignment data.RData")
#sona<-sona[-c(2,20),]  remove Motlante and De Klerk in this version of data split
sona<-sona%>% mutate(speech= str_replace(speech, "\\d{1,2} [A-Za-z]+ \\d{4}", "")) # Remove dates at the start of the speech
sona<- sona%>% mutate(speech= str_replace(speech, pattern = "^Thursday, ", replacement = ""))# remove dates on 2 remaining Ramaphosa speeches 
sona<- sona%>% mutate(speech= str_trim(speech, side= "left"))

Sona_S_tokenized<- unnest_tokens(sona, sentence, speech, token = 'sentences') 
Sona_S_tokenized<- Sona_S_tokenized%>% mutate(sentence= str_replace_all(sentence, "[[:punct:]]", ""))


```

The dataset is made up of speeches across a span of 28 years by the last 6 presidents of South Africa.

```{r Top 20 words}

Sona_tokenized<-unnest_tokens(sona, word, speech, token = 'words') 

Zuma<-Sona_tokenized%>% group_by(president_13, word)%>% 
        summarise("count"= n())%>% filter(president_13=="Zuma") %>%
        arrange(desc(count))%>%
        filter(!word %in% stop_words$word) %>%
        filter(rank(desc(count)) <= 15) %>%
        ggplot(aes(x = reorder(word, count), y = count)) + geom_col() + coord_flip() + xlab('Top 20 words used')+
   ggtitle("Zuma")+theme(plot.title = element_text(hjust=0.5))

Mbeki<-Sona_tokenized%>% group_by(president_13, word)%>% 
        summarise("count"= n())%>% filter(president_13=="Mbeki") %>%
        arrange(desc(count))%>%
        filter(!word %in% stop_words$word) %>%
        filter(rank(desc(count)) <= 15) %>%
        ggplot(aes(x = reorder(word, count), y = count)) + geom_col() + coord_flip() + xlab('Top 20 words used')+
   ggtitle("Mbeki")+theme(plot.title = element_text(hjust=0.5))

Mandela<-Sona_tokenized%>% group_by(president_13, word)%>% 
        summarise("count"= n())%>% filter(president_13=="Mandela") %>%
        arrange(desc(count))%>%
        filter(!word %in% stop_words$word) %>%
        filter(rank(desc(count)) <= 15) %>%
        ggplot(aes(x = reorder(word, count), y = count)) + geom_col() + coord_flip() + xlab('Top 20 words used')+
   ggtitle("Mandela")+theme(plot.title = element_text(hjust=0.5))

Ramaphosa<-Sona_tokenized%>% group_by(president_13, word)%>% 
        summarise("count"= n())%>% filter(president_13=="Ramaphosa") %>%
        arrange(desc(count))%>%
        filter(!word %in% stop_words$word) %>%
        filter(rank(desc(count)) <= 15) %>%
        ggplot(aes(x = reorder(word, count), y = count)) + geom_col() + coord_flip() + xlab('Top 20 words used')+
   ggtitle("Ramaphosa")+theme(plot.title = element_text(hjust=0.5))

DeKlerk<-Sona_tokenized%>% group_by(president_13, word)%>% 
        summarise("count"= n())%>% filter(president_13=="deKlerk") %>%
        arrange(desc(count))%>%
        filter(!word %in% stop_words$word) %>%
        filter(rank(desc(count)) <= 15) %>%
        ggplot(aes(x = reorder(word, count), y = count)) + geom_col() + coord_flip() + xlab('Top 20 words used')+
   ggtitle("deKlerk")+theme(plot.title = element_text(hjust=0.5))

Motlanthe<-Sona_tokenized%>% group_by(president_13, word)%>% 
        summarise("count"= n())%>% filter(president_13=="Motlanthe") %>%
        arrange(desc(count))%>%
        filter(!word %in% stop_words$word) %>%
        filter(rank(desc(count)) <= 15) %>%
        ggplot(aes(x = reorder(word, count), y = count)) + geom_col() + coord_flip() + xlab('Top 20 words used') + ggtitle("Motlanthe")+theme(plot.title = element_text(hjust=0.5))

#| label: fig-Words
#| fig-cap: "Top 20 words"
grid.arrange(DeKlerk,Mandela,Mbeki,Motlanthe,Zuma,Ramaphosa, ncol=3,nrow=2)
```

@fig-Words shows the most common words used by each president, outside of stop words that are common to all types of sentences construction. There is a visible tone change in the state of the nation addresses over time. With DeKlerk, a government centric tone is observed with high references to the constitution, parties and elections. The succeeding presidents put more emphasis on the betterment of the country, with Mandela and Mbeki speaking more on social welfare and Motlanthe, Zuma and Ramaphosa focusing on the country's economic development .

```{r}
#| fig-cap: "Average Sentence Length"
#| label: fig-sentences
Sona_S_tokenized %>%
  group_by(president_13) %>%
  summarise(avg_sentence_length = mean(n()))
```

## Methods

### Input Methods

#### Bag-of words

In the realm of natural language processing, Bag-of-Words (BoW) stands as a valuable text modeling tool. Its primary function is to facilitate the extraction of essential keywords from text while filtering out less critical information. The BoW process begins by establishing a vocabulary based on the words present in the text data. It then represents various speeches by quantifying the frequency of these keywords within each speech. The resulting dataset is an array of speeches against keywords, each entry reflecting the frequency of a specific keyword in a given speech. This foundational approach serves as the basis for machine learning models employed to classify speeches to the president that gave them. The machine learning models tradionally work with numerical data rather than textual data. Text data is unstructured and ML models can only operate with structured data that is well defined so by BoW technique the techniques can be integrated with more ease.

Notably, BoW has found applications in diverse contexts, such as a study in India where it was integrated with Convolutional Neural Networks to classify cases based on keyword analysis from case files. Remarkably, this study achieved an average accuracy rate of 85% (Pillai & Chandran, 2020). The following is a snippet of the BoW done on the SONA speeches. It does how the key words across all speeches become the columns of the array and the tally of each word in the two speeches is in their intersection.

```{r Word-bag}

identifiers<- 1:nrow(Sona_S_tokenized)
Sona_S_tokenized$Sent_ID=identifiers

word_bag<- Sona_S_tokenized%>% 
            unnest_tokens(input = sentence, output = word, token = 'words') %>%
            group_by(Sent_ID, president_13, word) %>% 
            summarise("count"=n())%>% filter(!word %in% stop_words$word)%>%
            top_n(200)

tweets_tdf<- Sona_S_tokenized %>%
  unnest_tokens(input = sentence, output = word, token = 'words') %>%
  inner_join(word_bag) %>%
  group_by(Sent_ID, president_13,word) %>%
  count() %>%  
  group_by(president_13) %>%
  mutate(total = sum(n)) %>%
  ungroup()



bag_of_words <- tweets_tdf %>% 
  select(Sent_ID, president_13, word, n) %>% 
  pivot_wider(names_from = word, values_from = n, values_fill = 0)%>%
  mutate(president_13= as.factor(president_13))


knitr::kable(bag_of_words[1:3,c(1,2,5:10)])
#| label: tbl-bag-of-words
#| tbl-cap: Snippet of Bag-of-Words
#| 
#test<-bag_of_words%>%semi_join(test_data, by= "filename")%>% select(-filename)
#train<-bag_of_words%>%semi_join(train_data, by= "filename")%>% select(-filename)

```

|            filename             | president | abroad | abuse | accept | achieve | acting |
|:---------:|:---------:|-----------|-----------|-----------|-----------|-----------|
| 1994_post_elections_Mandela.txt |  Mandela  | 3      | 2     | 2      | 4       | 2      |
| 1994_pre_elections_deKlerk.txt  |  deKlerk  | 0      | 1     | 1      | 2       | 0      |

: Snippet of Bag-of-Words model

#### Tf-Idf

### Predictive Models

#### Classification Trees

```{r}

test_data <- Sona_S_tokenized %>% group_by(president_13) %>% slice_sample(prop = 0.3) %>% ungroup()
train_data <- anti_join(Sona_S_tokenized, test_data, by = c("president_13", "Sent_ID"))

test<-bag_of_words%>%semi_join(test_data, by= "Sent_ID")%>% select(-Sent_ID)
train<-bag_of_words%>%semi_join(train_data, by= "Sent_ID")%>% select(-Sent_ID)

fit <- rpart(president_13 ~ ., train, method = 'class')
options(repr.plot.width = 12, repr.plot.height = 10) # set plot size in the notebook
plot(fit, main = 'Classification Tree')
text(fit, use.n = TRUE, all = TRUE, cex=.8)

fittedtrain <- predict(fit, type = 'class')
predtrain <- table(train$president_13, fittedtrain)
knitr:: kable(predtrain)

#round(sum(diag(predtrain))/sum(predtrain), 3)  training accuracy

fittedtest <- predict(fit, newdata = test, type = 'class')
predtest <- table(test$president_13, fittedtest)
knitr::kable(predtest)
#round(sum(diag(predtest))/sum(predtest), 3)  test accuracy


```

#### Random Forest

#### Neural Networks

## Results

## Discussion

## Conclusion

Potentially explore CNNs for even higher accuracy results -It is proven that CNN has significance in the image processing domain by classifying and identifying the object, but it is highly feasible for using CNN in NLP domains.(rephrase)
