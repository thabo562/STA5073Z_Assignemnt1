---
title: "STA5073Z Assignment 1"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(stringr)
library(dplyr)
library(tidyverse)
library(rpart)
library(gridExtra)
library(ggplot2) 

#install.packages("randomForest")
library(randomForest)
#install.packages("tidytext")
library(tidytext)

#update r and install git
#install.packages("remotes")
#remotes::install_github(sprintf("rstudio/%s",
#                        c("reticulate", "tensorflow","keras")))
#keras:: install_keras()
library(keras)
library(tensorflow)
#tf$constant("Hello world")
```

# Abstract

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

\*\*index words\*\* : Bag-of_words, Tf-IDF, Feed forward neural network (FFNN), random forest, classification tree

## Introduction

The aim of this paper is to identify different machine learning models that can be used for predictive purposes, and rank them according to their ability to do so. With presidential speech from 1994 to 2022 from South African presidents, this paper will review the capacity in which classification trees , feed forward neural networks and random forests are able to accurately classify sentences to their presidents. The input structuring process will also vary between bag of words and tf-idf so as to ascertain which one is more suitable for the data at hand with a selected machine learning model.

## Exploratory Data analysis

```{r Reading in Data}

set.seed(2022)
load("F:/STA5073Z- DS4I/pre-processed assignment data.RData")
load("~/STA5073Z_Assignemnt1/Assignement code workspace.RData")
#sona<-sona[-c(2,20),]  remove Motlante and De Klerk in this version of data split
sona<-sona%>% mutate(speech= str_replace(speech, "\\d{1,2} [A-Za-z]+ \\d{4}", "")) # Remove dates at the start of the speech
sona<- sona%>% mutate(speech= str_replace(speech, pattern = "^Thursday, ", replacement = ""))# remove dates on 2 remaining Ramaphosa speeches 
sona<- sona%>% mutate(speech= str_trim(speech, side= "left"))

Sona_S_tokenized<- unnest_tokens(sona, sentence, speech, token = 'sentences') 
Sona_S_tokenized<- Sona_S_tokenized%>% mutate(sentence= str_replace_all(sentence, "[[:punct:]]", ""))


```

The dataset is made up of speeches across a span of 28 years by the last 6 presidents of South Africa.

```{r Top 20 words}

Sona_tokenized<-unnest_tokens(sona, word, speech, token = 'words') 

Zuma<-Sona_tokenized%>% group_by(president_13, word)%>% 
        summarise("count"= n())%>% filter(president_13=="Zuma") %>%
        arrange(desc(count))%>%
        filter(!word %in% stop_words$word) %>%
        filter(rank(desc(count)) <= 15) %>%
        ggplot(aes(x = reorder(word, count), y = count)) + geom_col() + coord_flip() + xlab('Top 20 words used')+
   ggtitle("Zuma")+theme(plot.title = element_text(hjust=0.5))

Mbeki<-Sona_tokenized%>% group_by(president_13, word)%>% 
        summarise("count"= n())%>% filter(president_13=="Mbeki") %>%
        arrange(desc(count))%>%
        filter(!word %in% stop_words$word) %>%
        filter(rank(desc(count)) <= 15) %>%
        ggplot(aes(x = reorder(word, count), y = count)) + geom_col() + coord_flip() + xlab('Top 20 words used')+
   ggtitle("Mbeki")+theme(plot.title = element_text(hjust=0.5))

Mandela<-Sona_tokenized%>% group_by(president_13, word)%>% 
        summarise("count"= n())%>% filter(president_13=="Mandela") %>%
        arrange(desc(count))%>%
        filter(!word %in% stop_words$word) %>%
        filter(rank(desc(count)) <= 15) %>%
        ggplot(aes(x = reorder(word, count), y = count)) + geom_col() + coord_flip() + xlab('Top 20 words used')+
   ggtitle("Mandela")+theme(plot.title = element_text(hjust=0.5))

Ramaphosa<-Sona_tokenized%>% group_by(president_13, word)%>% 
        summarise("count"= n())%>% filter(president_13=="Ramaphosa") %>%
        arrange(desc(count))%>%
        filter(!word %in% stop_words$word) %>%
        filter(rank(desc(count)) <= 15) %>%
        ggplot(aes(x = reorder(word, count), y = count)) + geom_col() + coord_flip() + xlab('Top 20 words used')+
   ggtitle("Ramaphosa")+theme(plot.title = element_text(hjust=0.5))

DeKlerk<-Sona_tokenized%>% group_by(president_13, word)%>% 
        summarise("count"= n())%>% filter(president_13=="deKlerk") %>%
        arrange(desc(count))%>%
        filter(!word %in% stop_words$word) %>%
        filter(rank(desc(count)) <= 15) %>%
        ggplot(aes(x = reorder(word, count), y = count)) + geom_col() + coord_flip() + xlab('Top 20 words used')+
   ggtitle("deKlerk")+theme(plot.title = element_text(hjust=0.5))

Motlanthe<-Sona_tokenized%>% group_by(president_13, word)%>% 
        summarise("count"= n())%>% filter(president_13=="Motlanthe") %>%
        arrange(desc(count))%>%
        filter(!word %in% stop_words$word) %>%
        filter(rank(desc(count)) <= 15) %>%
        ggplot(aes(x = reorder(word, count), y = count)) + geom_col() + coord_flip() + xlab('Top 20 words used') + ggtitle("Motlanthe")+theme(plot.title = element_text(hjust=0.5))

#| label: fig-Words
#| fig-cap: "Top 20 words"
grid.arrange(DeKlerk,Mandela,Mbeki,Motlanthe,Zuma,Ramaphosa, ncol=3,nrow=2)
```

@fig-Words shows the most common words used by each president, outside of stop words that are common to all types of sentences construction. There is a visible tone change in the state of the nation addresses over time. With DeKlerk, a government centric tone is observed with high references to the constitution, parties and elections. The succeeding presidents put more emphasis on the betterment of the country, with Mandela and Mbeki speaking more on social welfare and Motlanthe, Zuma and Ramaphosa focusing on the country's economic development .

```{r}
#| fig-cap: "Average Sentence Length"
#| label: fig-sentences
Sona_S_tokenized %>%
  group_by(president_13) %>%
  summarise(avg_sentence_length = mean(n()))
```

## Methods

### Input Methods

#### Bag-of words

In the realm of natural language processing, Bag-of-Words (BoW) stands as a valuable text modeling tool. Its primary function is to facilitate the extraction of essential keywords from text while filtering out less critical information. The BoW process begins by establishing a vocabulary based on the words present in the text data. It then represents various speeches by quantifying the frequency of these keywords within each speech. The resulting dataset is an array of speeches against keywords, each entry reflecting the frequency of a specific keyword in a given speech. This foundational approach serves as the basis for machine learning models employed to classify speeches to the president that gave them. The machine learning models tradionally work with numerical data rather than textual data. Text data is unstructured and ML models can only operate with structured data that is well defined so by BoW technique the techniques can be integrated with more ease.

Notably, BoW has found applications in diverse contexts, such as a study in India where it was integrated with Convolutional Neural Networks to classify cases based on keyword analysis from case files. Remarkably, this study achieved an average accuracy rate of 85% (Pillai & Chandran, 2020). The following is a snippet of the BoW done on the SONA speeches. It does how the key words across all speeches become the columns of the array and the tally of each word in the two speeches is in their intersection.

```{r Word-bag}

identifiers<- 1:nrow(Sona_S_tokenized)
Sona_S_tokenized$Sent_ID=identifiers

#word_bag<- Sona_S_tokenized%>% 
            #unnest_tokens(input = sentence, output = word, token = 'words') %>%
           # group_by(Sent_ID, president_13, word) %>% 
            #summarise("count"=n())%>% filter(!word %in% stop_words$word)%>%
            #top_n(200)

tweets_tdf<- Sona_S_tokenized %>%
  unnest_tokens(input = sentence, output = word, token = 'words') %>%
  inner_join(word_bag) %>%
  group_by(Sent_ID, president_13,word) %>%
  count() %>%  
  group_by(president_13) %>%
  mutate(total = sum(n)) %>%
  ungroup()



bag_of_words <- tweets_tdf %>% 
  select(Sent_ID, president_13, word, n) %>% 
  pivot_wider(names_from = word, values_from = n, values_fill = 0)%>%
  mutate(president_13= as.factor(president_13))


knitr::kable(bag_of_words[1:3,c(1,2,5:10)])
#| label: tbl-bag-of-words
#| tbl-cap: Snippet of Bag-of-Words
#| 
#test<-bag_of_words%>%semi_join(test_data, by= "filename")%>% select(-filename)
#train<-bag_of_words%>%semi_join(train_data, by= "filename")%>% select(-filename)

```

#### Tf-Idf

Term Frequency- Inverse Document Frequency (TF_IDF) is another input method used for text modelling.  It is similar to  BoW in that it considers the number of times a word is used in a line of text however, it also factors in how important that word is relative to the other words in the document. The TF measures how frequently a term appears in a document by diving the total number of times the term appears in the document by the total number of terms within that document. This emphasizes the terms that occur more frequently than others. The IDF  measures uniqueness/rarity of a term across a collection of documents. It is calculated by taking the logarithm of the total number of documents in the collection divided by the number of documents containing the term. The logarithm is used to scale down the impact of common words. The two part of the statistical measure are calculated as follows:

-   Term Frequency:  (Number of times the term appears in the document) / (Total number of terms in the document)

-   Inverse Document Frequency= In(Total number of documents / Number of documents containing the term)

There have been studies conducted using both BoW and TF-IDF which found that TF-IDF generally out performed BoW when combined with ML models,  however this will not always be the cases because there are countless ML algorithms that exist and each one has a particular method that will allow for the best fit with a given dataset. For example, a study conducted to identify hate speech in tweets found that TF_IDF performed best with Decision trees while BoW performed best with logistic regression, each method obtaining relatively high accuracies. (Akuma et al., 2022)

### Predictive Models & Results

#### Classification Trees

```{r}

test_data <- Sona_S_tokenized %>% group_by(president_13) %>% slice_sample(prop = 0.3) %>% ungroup()
train_data <- anti_join(Sona_S_tokenized, test_data, by = c("president_13", "Sent_ID"))

test<-bag_of_words%>%semi_join(test_data, by= "Sent_ID")%>% select(-Sent_ID)
train<-bag_of_words%>%semi_join(train_data, by= "Sent_ID")%>% select(-Sent_ID)

fit <- rpart(president_13 ~ ., train, method = 'class')
options(repr.plot.width = 12, repr.plot.height = 10) # set plot size in the notebook
plot(fit, main = 'Classification Tree')
text(fit, use.n = TRUE, all = TRUE, cex=.8)

fittedtrain <- predict(fit, type = 'class')
predtrain <- table(train$president_13, fittedtrain)
#knitr:: kable(predtrain)
#round(sum(diag(predtrain))/sum(predtrain), 3)  training accuracy

```

Figure \[\] shows a classification tree when text is modelled by BoW. The tree does not have a lot of branches which is indicative of poor classification. The only two presidents can be classified are Mbeki and Zuma , which means every sentence put into the model will return either one of those presidents which is not true. The following table shows the classification using the test dataset:

```{r CT test set}
fittedtest <- predict(fit, newdata = test, type = 'class')
predtest <- table(test$president_13, fittedtest)
knitr::kable(predtest)
#round(sum(diag(predtest))/sum(predtest), 3)  test accuracy

```

The recorded classification rate of this model was 30% meaning the model The model is performing better than random guessing but still has significant room for improvement. The possible reasons for this low classification are unclear as it could be attributed to the model's performance itself or underlying issues with the data. As previously mentioned in the data analysis, the number of speeches each president makes is different with some making one and others making more than 5, causing a high degree of class imbalance.

#### Random Forest

Random Forest revolves around an ensemble of decision trees reinforced by bagging. Bagging, short for Bootstrap Aggregating, reduces variance and adds stability to these decision trees. It begins by creating multiple subsets of the training data, each of equal size to the original dataset, formed by random selection with potential repetition. These bootstrapped subsets are then used to independently build individual decision trees, each providing a unique perspective on the text data. Once the trees are constructed, they collectively contribute to the final prediction through a majority vote. This aggregation enhances robustness and accuracy, particularly beneficial for text data.

Additionally, Random Forest introduces random feature selection, where only a subset of features is considered at each node during tree construction, helping to reduce correlation between the trees and avoid overfitting, which is a common challenge when dealing with high-dimensional text data. Additionally, it offers the capability to assess variable importance by considering the random feature selection, providing insights into the crucial elements determining the president which said the given sentence. The figure below shows the error rate of the random forest as the number of trees increase.

```{r RF_BOW}

load("~/STA5073Z_Assignemnt1/Assignement code workspace.RData")

train<- as.data.frame(train)
#classifier_RF = randomForest(x = train[,-1], y = train[,1],ntree = 20) # commented out becuase html file takes too lon to render the document
  
plot(classifier_RF$err.rate[,1], xlab="Trees", ylab="Error rate", type="l") #view the output of the random forest

#| fig-cap: "BoW Random forest error rate"
#| label: fig-RF_BOW

```

as

```{r}
knitr:: kable(classifier_RF$confusion)
test<- as.data.frame(test)
y_pred = predict(classifier_RF, newdata = test[,-1]) 
  
# Confusion Matrix 
confusion_mtx = table(test[,1], y_pred) 
#confusion_mtx 
#round(sum(diag(confusion_mtx))/sum(confusion_mtx), 3) # test accuracy
```

For the presidents with fewer speeches in the dataset, such as deKlerk and Motlanthe, the error rates are notably high, with deKlerk reaching an error rate of 85.1% and Motlanthe experiencing a much more substantial error rate of 99.5%. Interestingly, this analysis reveals recurring patterns where a significant portion of deKlerk's speeches are misclassified as Zuma, and Motlanthe's speeches are predominantly classified as Mbeki.

On the other hand, for more prolific presidents like Mbeki and Zuma, the error rates are comparatively lower, with Mbeki achieving an error rate of 47.1% and Zuma having an error rate of 41.4%. However, it's worth noting that the error rate for Zuma's speeches has seen a notable increase in comparison to previous models, while Mbeki's error rate has seen a significant decrease. These observations shed light on the model's performance in handling classes with imbalanced data, indicating potential areas for improvement in correctly classifying the speeches of less frequently represented presidents.

```{r}
# Variable importance plot 
varImpPlot(classifier_RF) 
```

"\@fig-RF_VarImp," shows the top 30 most important features, each possessing a mean decrease Gini score exceeding 5. This score reflects the feature's contribution to the reduction of Gini impurity, with higher scores indicating greater importance in the model's predictive capability. The analysis has identified the two most influential words as 'compariots' and 'regard,' both exhibiting mean decrease Gini scores exceeding 30. Following ,with a score of almost half, are 'rand' and 'capture' with mean deacrease gini scores just below 15.

#### Neural Networks

## Discussion

## Conclusion

Potentially explore CNNs for even higher accuracy results -It is proven that CNN has significance in the image processing domain by classifying and identifying the object, but it is highly feasible for using CNN in NLP domains.(rephrase)
